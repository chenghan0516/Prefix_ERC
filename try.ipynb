{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_virtual_tokens: 7\n",
      "num_subjects: 2\n",
      "padding_idx: 6\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModelForSeq2SeqLM, PromptEncoderConfig, PromptEncoderReparameterizationType, get_peft_model, get_peft_model_state_dict, set_peft_model_state_dict\n",
    "from peft.tuners.prefix_tuning import PrefixEncoder\n",
    "from peft.tuners.p_tuning import PromptEncoder\n",
    "from peft.utils import _get_batch_size, PeftType, TaskType, TRANSFORMERS_MODELS_TO_PREFIX_TUNING_POSTPROCESS_MAPPING, map_cache_to_layer_device_map\n",
    "from transformers import PreTrainedModel, DynamicCache, EncoderDecoderCache\n",
    "from typing import Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class AbstractPromptEncoderConfig(PromptEncoderConfig):\n",
    "    \"\"\"\n",
    "    This is the configuration class to store the configuration of a [`PromptEncoder`].\n",
    "\n",
    "    Args:\n",
    "        encoder_reparameterization_type (Union[[`PromptEncoderReparameterizationType`], `str`]):\n",
    "            The type of reparameterization to use.\n",
    "        encoder_hidden_size (`int`): The hidden size of the prompt encoder.\n",
    "        encoder_num_layers (`int`): The number of layers of the prompt encoder.\n",
    "        encoder_dropout (`float`): The dropout probability of the prompt encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    num_subjects: int = field(\n",
    "        default=8,\n",
    "        metadata={\"help\": \"The number of subjects of the prompt encoder\"},\n",
    "    )\n",
    "    padding_idx: int = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The padding index of the prompt encoder\"},\n",
    "    )\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        self.peft_type = PeftType.P_TUNING #TODO: switch to APTuning\n",
    "\n",
    "\n",
    "class AbstractPromptEncoder(PromptEncoder):\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.num_subjects = config.num_subjects\n",
    "        self.total_virtual_tokens = config.num_virtual_tokens * config.num_subjects * config.num_transformer_submodules\n",
    "        if config.padding_idx is not None:\n",
    "            self.padding_idx = config.padding_idx\n",
    "        else:\n",
    "            self.padding_idx = self.total_virtual_tokens\n",
    "            self.total_virtual_tokens += 1\n",
    "\n",
    "        print(f\"total_virtual_tokens: {self.total_virtual_tokens}\")\n",
    "        print(f\"num_subjects: {self.num_subjects}\")\n",
    "        print(f\"padding_idx: {self.padding_idx}\")\n",
    "        # embedding\n",
    "        self.embedding = torch.nn.Embedding(self.total_virtual_tokens, self.token_dim, padding_idx=self.padding_idx)\n",
    "        if not config.inference_mode:\n",
    "            if self.encoder_type == PromptEncoderReparameterizationType.LSTM:\n",
    "                lstm_dropout = config.encoder_dropout\n",
    "                num_layers = config.encoder_num_layers\n",
    "                # LSTM\n",
    "                self.lstm_head = torch.nn.LSTM(\n",
    "                    input_size=self.input_size,\n",
    "                    hidden_size=self.hidden_size,\n",
    "                    num_layers=num_layers,\n",
    "                    dropout=lstm_dropout,\n",
    "                    bidirectional=True,\n",
    "                    batch_first=True,\n",
    "                )\n",
    "\n",
    "                self.mlp_head = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(self.hidden_size * 2, self.hidden_size * 2),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(self.hidden_size * 2, self.output_size),\n",
    "                )\n",
    "\n",
    "            elif self.encoder_type == PromptEncoderReparameterizationType.MLP:\n",
    "                encoder_num_layers_default = AbstractPromptEncoderConfig.encoder_num_layers\n",
    "                if config.encoder_num_layers != encoder_num_layers_default:\n",
    "                    warnings.warn(\n",
    "                        f\"for {self.encoder_type.value}, the argument `encoder_num_layers` is ignored. \"\n",
    "                        f\"Exactly {encoder_num_layers_default} MLP layers are used.\"\n",
    "                    )\n",
    "                layers = [\n",
    "                    torch.nn.Linear(self.input_size, self.hidden_size),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(self.hidden_size, self.output_size),\n",
    "                ]\n",
    "                self.mlp_head = torch.nn.Sequential(*layers)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.\")\n",
    "\n",
    "batch_size=2\n",
    "num_virtual_tokens = 3\n",
    "num_subjects = 2\n",
    "token_dim=5\n",
    "conversation_len = 3\n",
    "\n",
    "peft_config = AbstractPromptEncoderConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, num_virtual_tokens=num_virtual_tokens, num_subjects=num_subjects, token_dim=token_dim, num_transformer_submodules=1, encoder_hidden_size=10)\n",
    "\n",
    "temp = AbstractPromptEncoder(peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<pad>\n",
      "{'input_ids': [0, 1], 'attention_mask': [1, 1]}\n",
      "{'input_ids': [48, 19, 3, 9, 1622, 29, 1433, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [48, 19, 3, 9, 1622, 29, 1433, 1, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]}\n",
      "{'input_ids': [48, 19, 3, 9, 1622, 29, 1433, 0, 1, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.pad_token)\n",
    "print(tokenizer(\"<pad>\"))\n",
    "\n",
    "temp = \"this is a sentnence\"\n",
    "print(tokenizer(temp))\n",
    "print(tokenizer(temp, padding=\"max_length\", max_length=10))\n",
    "print(tokenizer(temp+\"<pad>\", padding=\"max_length\", max_length=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'Hi')]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m progressive_dataset = ProgressiveDialogueDataset(dialogues)\n\u001b[32m     41\u001b[39m dataloader = DataLoader(progressive_dataset, batch_size=\u001b[32m2\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\python\\CARC_stuff\\CARCVenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\python\\CARC_stuff\\CARCVenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\python\\CARC_stuff\\CARCVenv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\python\\CARC_stuff\\CARCVenv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[39m, in \u001b[36mdefault_collate\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault_collate\u001b[39m(batch):\n\u001b[32m    338\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[32m    340\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\python\\CARC_stuff\\CARCVenv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:207\u001b[39m, in \u001b[36mcollate\u001b[39m\u001b[34m(batch, collate_fn_map)\u001b[39m\n\u001b[32m    205\u001b[39m elem_size = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) == elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33meach element in list of batch should be of equal size\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    208\u001b[39m transposed = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(*batch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[31mRuntimeError\u001b[39m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ProgressiveDialogueDataset(Dataset):\n",
    "    def __init__(self, dialogue_data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dialogue_data: A list of lists, where each inner list contains sentences\n",
    "                           from one dialogue.\n",
    "        \"\"\"\n",
    "        self.dialogue_data = dialogue_data\n",
    "        self.cumulative_sentence_counts = [0]\n",
    "        self.row_lengths = [len(row) for row in dialogue_data]\n",
    "        total_sentences = 0\n",
    "        for length in self.row_lengths:\n",
    "            total_sentences += length\n",
    "            self.cumulative_sentence_counts.append(total_sentences)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_sentence_counts[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Find which row this index belongs to\n",
    "        row_index = 0\n",
    "        while self.cumulative_sentence_counts[row_index + 1] <= idx:\n",
    "            row_index += 1\n",
    "\n",
    "        # Find the index of the sentence within that row\n",
    "        sentence_index = idx - self.cumulative_sentence_counts[row_index]\n",
    "\n",
    "        # Get the progressive sequence of sentences\n",
    "        return self.dialogue_data[row_index][:sentence_index + 1]\n",
    "\n",
    "# Example Usage\n",
    "dialogues = [\n",
    "    [\"Hello\", \"How are you?\", \"I'm fine.\"],\n",
    "    [\"Hi\", \"What's up?\"],\n",
    "    [\"Greetings\"]\n",
    "]\n",
    "\n",
    "progressive_dataset = ProgressiveDialogueDataset(dialogues)\n",
    "dataloader = DataLoader(progressive_dataset, batch_size=2, shuffle=True, max_length)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Greetings']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progressive_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     77\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPrompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     79\u001b[39m config = AbstractPromptEncoderConfig(num_virtual_tokens=\u001b[32m3\u001b[39m, num_transformer_submodules=\u001b[32m1\u001b[39m, token_dim=\u001b[32m5\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m temp = \u001b[43mAbstractPromptEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mAbstractPromptEncoder.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mself\u001b[39m.num_subjects = config.num_subjects\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m.total_virtual_tokens = config.num_virtual_tokens * config.num_subjects * config.num_transformer_submodules\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\python\\CARC_stuff\\CARCVenv\\Lib\\site-packages\\peft\\tuners\\p_tuning\\model.py:110\u001b[39m, in \u001b[36mPromptEncoder.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.encoder_num_layers != encoder_num_layers_default:\n\u001b[32m    105\u001b[39m         warnings.warn(\n\u001b[32m    106\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.encoder_type.value\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, the argument `encoder_num_layers` is ignored. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExactly \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoder_num_layers_default\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MLP layers are used.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m         )\n\u001b[32m    109\u001b[39m     layers = [\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    111\u001b[39m         torch.nn.ReLU(),\n\u001b[32m    112\u001b[39m         torch.nn.Linear(\u001b[38;5;28mself\u001b[39m.hidden_size, \u001b[38;5;28mself\u001b[39m.hidden_size),\n\u001b[32m    113\u001b[39m         torch.nn.ReLU(),\n\u001b[32m    114\u001b[39m         torch.nn.Linear(\u001b[38;5;28mself\u001b[39m.hidden_size, \u001b[38;5;28mself\u001b[39m.output_size),\n\u001b[32m    115\u001b[39m     ]\n\u001b[32m    116\u001b[39m     \u001b[38;5;28mself\u001b[39m.mlp_head = torch.nn.Sequential(*layers)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\python\\CARC_stuff\\CARCVenv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:106\u001b[39m, in \u001b[36mLinear.__init__\u001b[39m\u001b[34m(self, in_features, out_features, bias, device, dtype)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mself\u001b[39m.in_features = in_features\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m.out_features = out_features\n\u001b[32m    105\u001b[39m \u001b[38;5;28mself\u001b[39m.weight = Parameter(\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m )\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[32m    109\u001b[39m     \u001b[38;5;28mself\u001b[39m.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
      "\u001b[31mTypeError\u001b[39m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AbstractPromptEncoderConfig(PromptEncoderConfig):\n",
    "    \"\"\"\n",
    "    This is the configuration class to store the configuration of a [`PromptEncoder`].\n",
    "\n",
    "    Args:\n",
    "        encoder_reparameterization_type (Union[[`PromptEncoderReparameterizationType`], `str`]):\n",
    "            The type of reparameterization to use.\n",
    "        encoder_hidden_size (`int`): The hidden size of the prompt encoder.\n",
    "        encoder_num_layers (`int`): The number of layers of the prompt encoder.\n",
    "        encoder_dropout (`float`): The dropout probability of the prompt encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    num_subjects: int = field(\n",
    "        default=8,\n",
    "        metadata={\"help\": \"The number of subjects of the prompt encoder\"},\n",
    "    )\n",
    "    padding_idx: int = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The padding index of the prompt encoder\"},\n",
    "    )\n",
    "    def __post_init__(self):\n",
    "        super().__post_init__()\n",
    "        self.peft_type = PeftType.P_TUNING #TODO: switch to APTuning\n",
    "\n",
    "\n",
    "class AbstractPromptEncoder(PromptEncoder):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_subjects = config.num_subjects\n",
    "        self.total_virtual_tokens = config.num_virtual_tokens * config.num_subjects * config.num_transformer_submodules\n",
    "        if config.padding_idx is not None:\n",
    "            self.padding_idx = config.padding_idx\n",
    "        else:\n",
    "            self.padding_idx = self.total_virtual_tokens\n",
    "            self.total_virtual_tokens += 1\n",
    "\n",
    "        # embedding\n",
    "        self.embedding = torch.nn.Embedding(self.total_virtual_tokens, self.token_dim, padding_idx=self.padding_idx)\n",
    "        if not config.inference_mode:\n",
    "            if self.encoder_type == PromptEncoderReparameterizationType.LSTM:\n",
    "                lstm_dropout = config.encoder_dropout\n",
    "                num_layers = config.encoder_num_layers\n",
    "                # LSTM\n",
    "                self.lstm_head = torch.nn.LSTM(\n",
    "                    input_size=self.input_size,\n",
    "                    hidden_size=self.hidden_size,\n",
    "                    num_layers=num_layers,\n",
    "                    dropout=lstm_dropout,\n",
    "                    bidirectional=True,\n",
    "                    batch_first=True,\n",
    "                )\n",
    "\n",
    "                self.mlp_head = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(self.hidden_size * 2, self.hidden_size * 2),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(self.hidden_size * 2, self.output_size),\n",
    "                )\n",
    "\n",
    "            elif self.encoder_type == PromptEncoderReparameterizationType.MLP:\n",
    "                encoder_num_layers_default = PromptEncoderConfig.encoder_num_layers\n",
    "                if config.encoder_num_layers != encoder_num_layers_default:\n",
    "                    warnings.warn(\n",
    "                        f\"for {self.encoder_type.value}, the argument `encoder_num_layers` is ignored. \"\n",
    "                        f\"Exactly {encoder_num_layers_default} MLP layers are used.\"\n",
    "                    )\n",
    "                layers = [\n",
    "                    torch.nn.Linear(self.input_size, self.hidden_size),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(self.hidden_size, self.output_size),\n",
    "                ]\n",
    "                self.mlp_head = torch.nn.Sequential(*layers)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Prompt encoder type not recognized. Please use one of MLP (recommended) or LSTM.\")\n",
    "\n",
    "config = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CARCVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
